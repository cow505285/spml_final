{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as Func\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myImageDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "    def __init__(self, root_dir, transform=None, classes = None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = classes\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 7000\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pic_class = index//500 + 1\n",
    "        pic_idx =  int(index%14) + 1\n",
    "        ori_img, target_img = self.get_img(pic_class, pic_idx), self.get_img(pic_class+1, pic_idx)\n",
    "        label = torch.tensor(int(index//500))\n",
    "        target_label = torch.tensor(int(index//500) + 1)\n",
    "        if self.transform is not None:\n",
    "            ori_img, target_img = self.transform(ori_img), self.transform(target_img)\n",
    "        return ori_img, target_img, label, target_label\n",
    "    \n",
    "    def get_img(self, pic_class, pic_idx):\n",
    "        pic_class =  '%03d'% pic_class\n",
    "        if pic_idx < 11:    \n",
    "            pic_idx = '%02d'% pic_idx\n",
    "            pic_name = f'/{pic_class}/frontal/{pic_idx}.jpg'\n",
    "        else:\n",
    "            pic_idx -= 10\n",
    "            pic_idx = '%02d'% pic_idx\n",
    "            pic_name = f'/{pic_class}/profile/{pic_idx}.jpg'\n",
    "        img_name = self.root_dir + pic_name\n",
    "        img = Image.open(img_name).convert('RGB')\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel,out_channel, kernel_size=3,stride=1,padding=1, groups=1, bias=False, dilation=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channel,out_channel, kernel_size=3,stride=1,padding=1, groups=1, bias=False, dilation=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        identity = x.detach_().clone()\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Block(nn.Module):\n",
    "    \"credits: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py#L4\"\n",
    "    def __init__(self, c, r=16):\n",
    "        super().__init__()\n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        bs, c, _, _ = x.shape\n",
    "        y = self.squeeze(x).view(bs, c)\n",
    "        y = self.excitation(y).view(bs, c, 1, 1)\n",
    "        return x * y.expand_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, image_channels, z_dim, layer_count):\n",
    "        super(encoder, self).__init__()\n",
    "        self.layer_count = layer_count\n",
    "        mul = 2\n",
    "        out_dim = 32\n",
    "        for i in range(layer_count):\n",
    "            setattr(self, f'conv_{i}', nn.Conv2d(image_channels, out_dim, kernel_size= 4, stride=2 , padding= 1))\n",
    "            setattr(self, f'batchnorm_2d_{i}', nn.BatchNorm2d(out_dim))\n",
    "            image_channels = out_dim\n",
    "            out_dim *= mul\n",
    "    def forward(self, x):\n",
    "        for i in range(self.layer_count):\n",
    "            x = getattr(self,f'conv_{i}')(x)\n",
    "            x = getattr(self,f'batchnorm_2d_{i}')(x)\n",
    "            x = nn.LeakyReLU()(x)\n",
    "        x = flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, z_dim, image_channels, layer_count, args):\n",
    "        super(decoder, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.layer_count = layer_count\n",
    "        self.fc3 = nn.Linear(z_dim, 4*4*args.context_size)\n",
    "        dev = 2\n",
    "        out_dim = z_dim // 2\n",
    "        for i in range(layer_count):\n",
    "            if i == layer_count - 1:\n",
    "                out_dim = 3\n",
    "            setattr(self, f'conv_trans_{i}', nn.ConvTranspose2d(z_dim, out_dim, kernel_size= 4, stride=2 , padding= 1))\n",
    "            setattr(self, f'de_batchnorm_2d_{i}', nn.BatchNorm2d(out_dim))\n",
    "            z_dim //= dev\n",
    "            out_dim //= dev\n",
    "        self.args = args\n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        z = self.fc3(x)\n",
    "        z = unflatten(z, self.args.context_size)\n",
    "        for i in range(self.layer_count):\n",
    "            z = getattr(self,f'conv_trans_{i}')(z)\n",
    "            z = getattr(self,f'de_batchnorm_2d_{i}')(z)\n",
    "            if i != self.layer_count - 1:\n",
    "                z = nn.LeakyReLU()(z)\n",
    "            else:\n",
    "                z = nn.Tanh()(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, args, image_channels=3, z_dim=256, layer_count = 4, is_g1 = False):\n",
    "        super(VAE, self).__init__()\n",
    "        self.layer_count = layer_count\n",
    "        self.fc1 = nn.Linear(4*4*256, z_dim)\n",
    "        self.fc2 = nn.Linear(4*4*256, z_dim)\n",
    "        self.encoder = encoder(image_channels, z_dim, layer_count)\n",
    "        self.decoder = decoder(z_dim, args.channel_size, layer_count, args)\n",
    "        self.generator = nn.Conv2d(6,3,3,padding=1)\n",
    "        self.basic_block = BasicBlock(image_channels,image_channels)\n",
    "        self.args = args\n",
    "        self.image_channels = image_channels\n",
    "        self.is_g1 = is_g1\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            self.normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    def normal_init(self, m, mean, std):\n",
    "        if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(mean, std)\n",
    "            m.bias.data.zero_()\n",
    "        \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp.to(self.args.device)    \n",
    "        return z\n",
    "    \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        logvar = Func.log_softmax(logvar, dim = -1)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    def representation(self, x):\n",
    "        return self.bottleneck(self.encoder(x))[0]\n",
    "\n",
    "    def forward(self, ori_x, target_x):\n",
    "        resi_x = self.basic_block(target_x)\n",
    "        attn_x = self.attn(resi_x).to(self.args.device)\n",
    "        enc_x = self.encoder(attn_x)\n",
    "        z, mu, logvar = self.bottleneck(enc_x)\n",
    "        recon_x_from_z = self.decoder(z)\n",
    "        if not self.is_g1:\n",
    "            return recon_x_from_z, None\n",
    "        \n",
    "        \n",
    "        vae_loss, MSE, KLD  = self.loss_fn(recon_x_from_z, target_x, mu, logvar)\n",
    "        \n",
    "        concat_feat = torch.cat((ori_x, recon_x_from_z),dim = 1)\n",
    "        recon_x = self.generator(concat_feat)\n",
    "        return recon_x ,vae_loss\n",
    "    \n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def attn(self, inputs):\n",
    "        N, C, H, W = self.args.batch_size, self.image_channels, self.args.input_size, self.args.input_size\n",
    "        patches = Func.unfold(inputs, (H//2, W//2), stride = 64).reshape(N,C,-1)\n",
    "        m_inputs = inputs.view(N,C,-1).permute(0,2,1)\n",
    "        weight = torch.bmm(m_inputs, patches)\n",
    "        resi_inputs = torch.bmm(weight, patches.reshape(N,-1,C))\n",
    "        inputs = torch.add(inputs, resi_inputs.reshape(N,C,H,W))\n",
    "        return inputs\n",
    "    \n",
    "    def loss_fn(self, recon_x, x , mu, logvar):\n",
    "        MSE = Func.mse_loss(recon_x, x)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp()) * args.kld_weight \n",
    "        return MSE + KLD, MSE, KLD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        self.classifier = nn.Sequential(nn.Linear(1000,200),nn.BatchNorm1d(200),nn.ReLU(),nn.Linear(200,1))\n",
    "    def forward(self,input):\n",
    "        feat = self.feature_extractor(input)\n",
    "        pred = self.classifier(feat)\n",
    "        return pred\n",
    "        \n",
    "\n",
    "class Aaan(nn.Module):\n",
    "    \"\"\"docstring for ClassName\"\"\"\n",
    "    def __init__(self, args, image_channels=3, z_dim=256, layer_count = 4):\n",
    "        super(Aaan, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.pair_loss = nn.PairwiseDistance()\n",
    "        self.z_dim = z_dim\n",
    "        self.vae = VAE(args, z_dim = z_dim, is_g1 = True)\n",
    "#         self.G1 =  VAE(args, z_dim = z_dim, is_g = True)\n",
    "        self.G2 =  VAE(args, z_dim = z_dim)\n",
    "        self.Identity_Discriminator = Discriminator()\n",
    "        self.Sample_Discriminator = Discriminator()\n",
    "        self.image_channels = image_channels\n",
    "        self.args = args\n",
    "        \n",
    "\n",
    "    def forward(self, origin_face, target_face, train_d):\n",
    "        result = namedtuple('result', [\"construct_image\",\"adv_loss\",\"rec_loss\",\"cos_loss\"])\n",
    "        adv_loss, rec_loss, cos_loss = 0, 0, 0\n",
    "        construct, vae_loss  = self.vae(origin_face, target_face)\n",
    "        adv_loss += vae_loss\n",
    "#         _, _, construct, _ = self.G1(concat_feat)\n",
    "        back_construct, _ = self.G2(None, construct)\n",
    "        rec_loss += self.loss(origin_face, back_construct) \n",
    "        cos_loss += self.cos_loss(target_face, construct)\n",
    "#         reconstruct_loss = self.l1_loss(origin_face, back_construct) + self.loss(construct, origin_face) \n",
    "        if train_d == 1:\n",
    "            Identity_dloss = self.calculate_identity_loss(construct,target_face)\n",
    "            Sample_dloss = self.calculate_sample_loss(construct,origin_face)\n",
    "            adv_loss += Identity_dloss+Sample_dloss\n",
    "        else:\n",
    "            identity_pred = self.Identity_Discriminator(construct)\n",
    "            real_pred = self.Sample_Discriminator(construct)\n",
    "            labels = torch.ones(identity_pred.size()).to(self.args.device)\n",
    "            adv_loss += self.loss(identity_pred.sigmoid(),labels)+ self.loss(real_pred.sigmoid(),labels)\n",
    "        output = result(construct, adv_loss, 10 * rec_loss, 5 * cos_loss)\n",
    "        return output\n",
    "\n",
    "    def calculate_identity_loss(self,construct,target_face):\n",
    "        origin_pred = self.Identity_Discriminator(construct)\n",
    "        target_pred = self.Identity_Discriminator(target_face)\n",
    "        origin_label = torch.zeros(origin_pred.size()).to(self.args.device)\n",
    "        target_label = torch.ones(target_pred.size()).to(self.args.device)\n",
    "        ori_loss = self.loss(origin_pred.sigmoid(),origin_label)\n",
    "        target_loss = self.loss(target_pred.sigmoid(),target_label)\n",
    "        Identity_dloss = (ori_loss + target_loss)\n",
    "        return Identity_dloss\n",
    "    def calculate_sample_loss(self,construct,origin_face):\n",
    "        construct_pred = self.Sample_Discriminator(construct)\n",
    "        origin_pred = self.Sample_Discriminator(origin_face)\n",
    "        construct_label = torch.zeros(construct_pred.size()).to(self.args.device)\n",
    "        origin_label = torch.ones(origin_pred.size()).to(self.args.device)\n",
    "        \n",
    "        fake_loss = self.loss(construct_pred.sigmoid(),construct_label)\n",
    "        ori_loss = self.loss(origin_pred.sigmoid(),origin_label)\n",
    "        Identity_dloss = (fake_loss + ori_loss)\n",
    "        return Identity_dloss\n",
    "    def cos_loss(self, x1, x2):\n",
    "        x1 = x1.view(3, -1)\n",
    "        x2 = x2.view(3, -1)\n",
    "#         return -Func.cosine_similarity(x1, x2).abs().mean()\n",
    "        return 1 - Func.cosine_similarity(x1, x2, 1, 1e-8).sum()\n",
    "#         return self.pair_loss(x1,x2).mean()\n",
    "    \n",
    "    def generate_adv(self, origin_face, target_face):\n",
    "        construct, vae_loss  = self.vae(origin_face, target_face)\n",
    "        return construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand([64, 3, 64, 64]).reshape(64,3,-1)\n",
    "b = torch.rand([64, 3, 64, 64]).reshape(64,3,-1)\n",
    "Func.cosine_similarity(a, b, 1, 1e-8).sum(-1).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    return x.view(x.size(0), -1)\n",
    "def unflatten(x, size):\n",
    "    return x.view(x.size(0), -1, 4, 4)\n",
    "\n",
    "def get_optimizer(model, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay = 1e-3,lr = lr)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.epoch = 100\n",
    "        self.batch_size = 32\n",
    "        self.lr = 1e-3\n",
    "        self.gpu = torch.cuda.is_available()\n",
    "        self.device = 1\n",
    "        self.input_size = 64\n",
    "        self.channel_size = 3\n",
    "        self.context_size = 128\n",
    "        self.kld_weight = 1e-5\n",
    "        self.alter_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "root_dir = 'dataset/cfp-dataset/Data/Images'\n",
    "text_file = 'dataset/cfp-dataset/Data/list_name.txt'\n",
    "classes = []\n",
    "with open (text_file, 'r') as f:\n",
    "    for i in f.read().split('\\n'):\n",
    "        classes.append(i)\n",
    "\n",
    "mean, std = [0.5], [0.5]\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((args.input_size, args.input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "        ])\n",
    "dataset = myImageDataset(root_dir, transform, classes )\n",
    "\n",
    "trainloader = DataLoader(dataset, batch_size = args.batch_size, drop_last = True, shuffle= True, worker_init_fn=0, )\n",
    "a3n = Aaan(args, z_dim = args.context_size)\n",
    "a3n.train()\n",
    "optimizer = get_optimizer(a3n, args.lr)\n",
    "if args.gpu:\n",
    "    a3n = a3n.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 4, iteration: 116 / 218,total_loss: 0.0318293571472168, adv_loss_d: 1.136741042137146, recon_loss:0.37527576088905334, cos_loss: -0.329930186271667521234\r"
     ]
    }
   ],
   "source": [
    "mode = {0:'g', 1:'d'}\n",
    "train_d = 1\n",
    "for epo in range(args.epoch):\n",
    "    index = 0\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    for param in trainloader:        \n",
    "        if args.gpu:\n",
    "            ori_img, target_img, label, target_label = [i.to(args.device) for i in param]\n",
    "        if index % args.alter_step <= 30:\n",
    "            train_d = 1\n",
    "        else:\n",
    "            train_d = 0\n",
    "        \n",
    "        output = a3n(ori_img, target_img, train_d)\n",
    "        loss =  sum(output[1:4] )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total += len(ori_img)\n",
    "        total_loss += loss.detach_().cpu()\n",
    "        print(f'epoch : {epo}, iteration: {index + 1} / {len(trainloader)},total_loss: {total_loss / total}, adv_loss_{mode[train_d]}: {output[1]}, recon_loss:{output[2]}, cos_loss: {output[3]}', end = '\\r')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3n.eval()\n",
    "for param in trainloader:\n",
    "    if args.gpu:\n",
    "        ori_img, target_img, label, target_label = [i.to(args.device) for i in param]\n",
    "    with torch.no_grad():\n",
    "        recon_images = a3n.generate_adv(ori_img, target_img)\n",
    "#     recon_images, loss = vae(img)\n",
    "    break\n",
    "rev_tran = transforms.Compose([\n",
    "    transforms.Normalize(mean=(-1.0,-1.0,-1.0),std = (2.0,2.0,2.0)),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "fig = plt.figure(figsize = (8,4))\n",
    "fig.add_subplot(1,3,1)\n",
    "plt.imshow(rev_tran(recon_images[0].cpu()))\n",
    "fig.add_subplot(1,3,2)\n",
    "plt.imshow(rev_tran(ori_img[0].cpu()))\n",
    "plt.show()\n",
    "fig.add_subplot(1,3,3)\n",
    "plt.imshow(rev_tran(target_img[0].cpu()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peter",
   "language": "python",
   "name": "peter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
